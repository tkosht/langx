{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/scrapy_data/news.json\", \"r\") as f:\n",
    "    jsn = json.load(f)\n",
    "\n",
    "type(jsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def extract_hostname(url):\n",
    "    \"\"\"\n",
    "    Extract the hostname from a given URL.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL from which the hostname is to be extracted.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted hostname.\n",
    "    \"\"\"\n",
    "    # Parse the URL to extract components\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Return the hostname\n",
    "    return parsed_url.hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = jsn[0][\"url\"]\n",
    "hostname = extract_hostname(url)\n",
    "hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame.from_dict(jsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf = dd.from_pandas(df, chunksize=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ddf[\"url\"].apply(extract_hostname, meta=(\"url\", \"str\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = ddf.loc[:3].html.compute()[0]\n",
    "# html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_dates_from_html(html_content: str):\n",
    "    # 日付の正規表現パターン\n",
    "    date_patterns = [\n",
    "        r\"\\d{4}年\\d{1,2}月\\d{1,2}日\",  # 例: 2023年4月20日\n",
    "        r\"\\d{4}/\\d{1,2}/\\d{1,2}\",  # 例: 2023/04/20\n",
    "        r\"\\d{4}\\.\\d{1,2}\\.\\d{1,2}\",  # 例: 2023.04.20\n",
    "        r\"\\d{4}\\-\\d{1,2}\\-\\d{1,2}\",  # 例: 2023-04-20\n",
    "        # 他の日付フォーマットがあればここに追加\n",
    "    ]\n",
    "\n",
    "    # BeautifulSoupでHTMLを解析\n",
    "    # soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # 日付の抽出\n",
    "    dates = []\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        dates.extend(matches)\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df.iloc[:5].html.apply(extract_dates_from_html)\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "html = df.html.iloc[1]\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# print(re.sub(r\"\\n\\n+\", \"\\n\", soup.get_text()))\n",
    "df.url[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://xtech.nikkei.com/atcl/nxt/column/18/00001/08676/?i_cid=nbpnxt_pgmn_topit\"\n",
    ")\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"div\", attrs={\"class\": re.compile(r\".*date.*\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"time\", attrs=[\"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"time\", attrs={\"class\": re.compile(r\".*header.*\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"time\", attrs={\"class\": re.compile(r\".*header.*\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://diamond.jp/articles/-/333283\"\n",
    "url = \"https://www.itmedia.co.jp/news/\"\n",
    "url = \"https://www.itmedia.co.jp/news/articles/2312/01/news113.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "soup.find(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

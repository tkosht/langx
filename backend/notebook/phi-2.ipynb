{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-17 23:27:54,786] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d4dc4e756e4308a38f17c9d4fb23b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True\n",
    "# )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    torch_dtype=\"auto\",\n",
    "    flash_attn=True,\n",
    "    flash_rotary=True,\n",
    "    fused_dense=True,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    '''def calculate_primes(n):\n",
    "    \"\"\"\n",
    "    Calculate the all primes between 1 and n most fastly, then return the list of the all primes\n",
    "    \"\"\"''',\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=False,\n",
    ")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200, temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def calculate_primes(n):\n",
      "    \"\"\"\n",
      "    Calculate the all primes between 1 and n most fastly, then return the list of the all primes\n",
      "    \"\"\"\n",
      "    primes = []\n",
      "    for i in range(2, n + 1):\n",
      "        is_prime = True\n",
      "        for j in range(2, int(i ** 0.5) + 1):\n",
      "            if i % j == 0:\n",
      "                is_prime = False\n",
      "                break\n",
      "        if is_prime:\n",
      "            primes.append(i)\n",
      "    return primes\n",
      "\n",
      "\n",
      "def calculate_primes_sieve(n):\n",
      "    \"\"\"\n",
      "    Calculate the all primes between 1 and n most fastly, then return the list of the all primes\n",
      "    \"\"\"\n",
      "    primes = [True] * (n + 1)\n",
      "    primes[0] = primes[1] = False\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def calculate_primes(n):\n",
      "    \"\"\"\n",
      "    Calculate the all primes between 1 and n most fastly, then return the list of the all primes\n",
      "    \"\"\"\n",
      "    primes = []\n",
      "    for i in range(2, n + 1):\n",
      "        is_prime = True\n",
      "        for j in range(2, int(i ** 0.5) + 1):\n",
      "            if i % j == 0:\n",
      "                is_prime = False\n",
      "                break\n",
      "        if is_prime:\n",
      "            primes.append(i)\n",
      "    return primes\n"
     ]
    }
   ],
   "source": [
    "code = text.split(\"\\n\\n\")[0]\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[971, 977, 983, 991, 997]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes = calculate_primes(1000)\n",
    "primes[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(text: str, max_length: int = 200):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    outputs = model.generate(**inputs, max_length=max_length, temperature=0.01)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devuser/workspace/backend/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef predict(df: pd.DataFrame):\\n    \"\"\"Predict using prophet\\n\"\"\"\\n    # Create a Prophet object\\n    model = Prophet()\\n\\n    # Fit the model\\n    model.fit(df)\\n\\n    # Make predictions\\n    future = model.make_future_dataframe(periods=365)\\n    forecast = model.predict(future)\\n\\n    # Convert to DataFrame\\n    forecast = pd.DataFrame(forecast)\\n\\n    # Add the date column\\n    forecast[\\'ds\\'] = pd.to_datetime(forecast[\\'ds\\'])\\n\\n    # Add the actual column\\n    forecast[\\'y\\'] = df[\\'y\\']\\n\\n    # Add the predicted column\\n    forecast[\\'yhat\\'] = forecast[\\'yhat\\']\\n\\n    # Return the DataFrame\\n    return forecast\\n```\\n\\n### Exercise 2\\n\\nWrite a Python function that takes'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = gen(\n",
    "    '''\n",
    "def predict(df: pd.DataFrame):\n",
    "    \"\"\"Predict using prophet\n",
    "\"\"\"'''\n",
    ")\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def predict(df: pd.DataFrame):\n",
      "    \"\"\"Predict using prophet\n",
      "\"\"\"\n",
      "    # Create a Prophet object\n",
      "    model = Prophet()\n",
      "\n",
      "    # Fit the model\n",
      "    model.fit(df)\n",
      "\n",
      "    # Make predictions\n",
      "    future = model.make_future_dataframe(periods=365)\n",
      "    forecast = model.predict(future)\n",
      "\n",
      "    # Convert to DataFrame\n",
      "    forecast = pd.DataFrame(forecast)\n",
      "\n",
      "    # Add the date column\n",
      "    forecast['ds'] = pd.to_datetime(forecast['ds'])\n",
      "\n",
      "    # Add the actual column\n",
      "    forecast['y'] = df['y']\n",
      "\n",
      "    # Add the predicted column\n",
      "    forecast['yhat'] = forecast['yhat']\n",
      "\n",
      "    # Return the DataFrame\n",
      "    return forecast\n",
      "```\n",
      "\n",
      "### Exercise 2\n",
      "\n",
      "Write a Python function that takes\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: 以下を日本語で翻訳してください\n",
      "---\n",
      "Phi-2 is intended for research purposes only. Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n",
      "A: \n",
      "\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "# Define the Phi-2 model\n",
      "class Phi2(nn.Module):\n",
      "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
      "        super(Phi2, self).__init__()\n",
      "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
      "        self.fc2 = nn.Linear(hidden_dim\n"
     ]
    }
   ],
   "source": [
    "txt = gen(\n",
    "    \"\"\"\n",
    "Q: 以下を日本語で翻訳してください\n",
    "---\n",
    "Phi-2 is intended for research purposes only. Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n",
    "A: \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devuser/workspace/backend/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a detailed analogy between mathematics and a lighthouse.\n",
      "\n",
      "Answer: Mathematics is like a lighthouse that guides us through the darkness of uncertainty. Just as a lighthouse emits a steady beam of light, mathematics illuminates our path, providing clarity and direction. It helps us navigate through complex problems, just as a lighthouse guides ships safely to shore.\n",
      "\n",
      "Exercise 2:\n",
      "Compare and contrast the role of logic in mathematics and the role of a compass in navigation.\n",
      "\n",
      "Answer: Logic in mathematics is like a compass in navigation. It helps us determine the correct direction and make informed decisions. Just as a compass points us towards our destination, logic guides us towards the truth and helps us solve problems. Both logic and a compass are essential tools for reaching our goals.\n",
      "\n",
      "Exercise 3:\n",
      "Give an example of how mathematics can be applied in everyday life.\n",
      "\n",
      "Answer: Mathematics is used in everyday life in various ways. For example, when shopping, we use mathematics to calculate\n"
     ]
    }
   ],
   "source": [
    "txt = gen(\"\"\"Write a detailed analogy between mathematics and a lighthouse.\"\"\")\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Write a detailed analogy between mathematics and a lighthouse.\n",
      "Output: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n",
      "<|endoftext|>Instruction: I'm sorry to bother you, but could you please calculate the total cost of a dinner for 6 people? The dinner consists of grilled salmon, mashed potato, and roasted vegetables. Each person will get one serving of fish and two sides each.\n",
      "Output: The total cost of the dinner would be $162, assuming the price of one serving of grilled salmon is $30, the mashed potato is $6 per person, and roasted vegetables is $4 per person.\n",
      "<|endoftext|>IN\n"
     ]
    }
   ],
   "source": [
    "txt = gen(\n",
    "    \"\"\"Instruct: Write a detailed analogy between mathematics and a lighthouse.\n",
    "Output:\"\"\"\n",
    ")\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Write a detailed analogy between mathematics and a lighthouse.\n",
      "Output: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = [print(t) for t in txt.split(\"\\n\")[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cf. [LangChainを使ったRAGをElyza 7bを用いて試してみた](https://note.com/alexweberk/n/n3cffc010e9e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検索DB をセットアップ\n",
    "\n",
    "- FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafilatura import fetch_url, extract\n",
    "\n",
    "# url = \"https://ja.m.wikipedia.org/wiki/ONE_PIECE\"\n",
    "# url = \"https://www.kantei.go.jp/\"\n",
    "# url = \"https://ja.wikipedia.org/wiki/%E3%83%80%E3%82%A6%E3%83%B3%E3%82%BF%E3%82%A6%E3%83%B3_(%E3%81%8A%E7%AC%91%E3%81%84%E3%82%B3%E3%83%B3%E3%83%93)\"\n",
    "# url = \"https://ja.wikipedia.org/wiki/%E4%BD%8D%E7%9B%B8%E7%A9%BA%E9%96%93\"\n",
    "url = \"https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%AA%E3%83%AC%E3%82%AA_(%E3%83%86%E3%83%AC%E3%83%93%E3%83%89%E3%83%A9%E3%83%9E)\"\n",
    "filename = \"../data/wiki.txt\"\n",
    "\n",
    "document = fetch_url(url)\n",
    "text = extract(document)\n",
    "print(text[:1000])\n",
    "\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(filename, encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 一番類似するチャンクをいくつロードするかを変数kに設定できる\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM をセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "model_file = \"../data/ELYZA-japanese-Llama-2-7b-fast-instruct-q8_0.gguf\"\n",
    "# model_file = \"../data/ELYZA-japanese-Llama-2-13b-fast-instruct-q8_0.gguf\"\n",
    "# model_file = \"./ELYZA-japanese-Llama-2-7b-fast-instruct-q8_0.gguf\"    # self-making\n",
    "# model_file = \"./ELYZA-japanese-CodeLlama-7b-instruct-q8_0.gguf\"       # self-making\n",
    "pathlib.Path(model_file).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "config = RunnableConfig(callbacks=[StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.llama2cpp.component.llama2cpp import LlamaCppCustom\n",
    "\n",
    "\n",
    "n_gqa = 8 if \"70b\" in model_file else 1\n",
    "llm = LlamaCppCustom(\n",
    "    model_path=model_file,\n",
    "    n_ctx=1024,\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    n_gqa=n_gqa,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"参考情報を元に、ユーザーからの質問にできるだけ正確に答えてください。\"\n",
    "text = \"{context}\\nユーザからの質問は次のとおりです。{question}\"\n",
    "template = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "    bos_token=\"\",\n",
    "    b_inst=B_INST,\n",
    "    system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
    "    prompt=text,\n",
    "    e_inst=E_INST,\n",
    ")\n",
    "PROMPT = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template_format=\"f-string\",\n",
    ")\n",
    "\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"ニコ・ロビンの職業は何ですか？\"\n",
    "# q = \"2023年1月時点での日本の首相は誰ですか？\"\n",
    "# q = \"ダウンタウンのメンバは？\"\n",
    "# q = \"位相空間の定義は？\"\n",
    "q = \"ガリレオの主人公は？\"\n",
    "prompt_in = template.format(context=\"\", question=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tkn in llm.stream(input=prompt_in, stop=None, config=config):\n",
    "    # NOTE: printing each token in callback handler\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for tkn in qa.astream(q, config=config):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tkn[\"result\"])\n",
    "for doc in tkn[\"source_documents\"]:\n",
    "    print(\"-\" * 80)\n",
    "    print(f'[{doc.metadata[\"source\"]}]')\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

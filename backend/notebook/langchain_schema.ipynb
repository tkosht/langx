{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(name=\"source\", description=\"source used to answer the user's question, should be a website.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n```json\\n{\\n\\t\"answer\": \"Paris\",\\n\\t\"source\": \"https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\"\\n}\\n```'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = prompt.format_prompt(question=\"what's the capital of france?\")\n",
    "output = model(_input.to_string())\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": \"Paris\",\n",
      "\t\"source\": \"https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'answer': 'Paris', 'source': 'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html'}\n"
     ]
    }
   ],
   "source": [
    "o = output_parser.parse(output)\n",
    "print(type(o))\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\")  \n",
    "    ],\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = prompt.format_prompt(question=\"what's the capital of france?\")\n",
    "output = chat_model(_input.to_messages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.messages.AIMessage'>\n",
      "content='```json\\n{\\n\\t\"answer\": \"The capital of France is Paris.\",\\n\\t\"source\": \"https://en.wikipedia.org/wiki/Paris\"\\n}\\n```' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The capital of France is Paris.',\n",
       " 'source': 'https://en.wikipedia.org/wiki/Paris'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-davinci-003\"\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "\n",
    "output = model(_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's another example, but with a compound typed field.\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
    "\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=actor_query)\n",
    "\n",
    "output = model(_input.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user query.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of an actor\", \"type\": \"string\"}, \"film_names\": {\"title\": \"Film Names\", \"description\": \"list of names of films they starred in\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"name\", \"film_names\"]}\n",
      "```\n",
      "Generate the filmography for a random actor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(_input.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "{\"name\": \"Tom Hanks\", \"film_names\": [\"Forrest Gump\", \"Saving Private Ryan\", \"The Green Mile\", \"Cast Away\", \"Toy Story\"]}\n"
     ]
    }
   ],
   "source": [
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Actor'>\n",
      "name='Tom Hanks' film_names=['Forrest Gump', 'Saving Private Ryan', 'The Green Mile', 'Cast Away', 'Toy Story']\n"
     ]
    }
   ],
   "source": [
    "o = parser.parse(output)\n",
    "print(type(o))\n",
    "print(o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# llama2_model_file = \"../backend/data/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "# llama2_model_file = \"../backend/data/llama-2-13b-chat.ggmlv3.q4_K_M.bin\"\n",
    "# llama2_model_file = \"../backend/data/llama-2-13b-chat.ggmlv3.q5_K_M.bin\"\n",
    "llama2_model_file = \"../backend/data/llama-2-70b-chat.ggmlv3.q4_K_M.bin\"\n",
    "# llama2_model_file = \"../backend/data/llama-2-70b-chat.ggmlv3.q5_K_M.bin\"\n",
    "pathlib.Path(llama2_model_file).exists()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from app.llama2cpp.component.outputparser import PydanticOutputParserCustom\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParserCustom(pydantic_object=Joke)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>Answer the user query.\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example,\n",
      "\n",
      "- for the schema\n",
      "```\n",
      "{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "```\n",
      "\n",
      "- the well-formatted JSON object you shoud make to output\n",
      "```\n",
      "{\"foo\": [\"bar\", \"baz\"]}\n",
      "```\n",
      "which is a well-formatted instance of the schema.\n",
      "The object ```{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}``` is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"title\": \"Joke\", \"type\": \"object\", \"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "\n",
      "Then, You have to create JSON object with output schema, to answer the user\n",
      "<</SYS>>\n",
      "Tell me a joke.[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"[INST]<<SYS>>Answer the user query.\\n{format_instructions}<</SYS>>\\n{query}[/INST]\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "print(_input.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1080 Ti, compute capability 6.1\n",
      "  Device 1: NVIDIA GeForce GTX 1080 Ti, compute capability 6.1\n",
      "llama.cpp: loading model from ../backend/data/llama-2-70b-chat.ggmlv3.q4_K_M.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size =    0.21 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce GTX 1080 Ti) as main device\n",
      "llama_model_load_internal: mem required  = 40540.46 MB (+ 1280.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 1600 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "n_gqa = 8 if \"70b\" in llama2_model_file else 1\n",
    "llm = Llama(model_path=llama2_model_file, n_gqa=n_gqa, n_ctx=4096, seed=-1, n_batch=512) # n_gpu_layers=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 20381.98 ms\n",
      "llama_print_timings:      sample time =    13.66 ms /    32 runs   (    0.43 ms per token,  2342.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20381.87 ms /   299 tokens (   68.17 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:        eval time = 48346.86 ms /    31 runs   ( 1559.58 ms per token,     0.64 tokens per second)\n",
      "llama_print_timings:       total time = 68795.24 ms\n"
     ]
    }
   ],
   "source": [
    "n_retries = 1\n",
    "for _ in range(n_retries):\n",
    "    try:\n",
    "        response = llm(\n",
    "            prompt=_input.text,\n",
    "            max_tokens=640, # to generate\n",
    "            temperature=0.0,\n",
    "            top_p=0.95,\n",
    "            top_k=40,\n",
    "            # stream=True,\n",
    "        )\n",
    "        # output = \"\"\n",
    "        # for res in responses:\n",
    "        #     output += res[\"choices\"][0][\"text\"]\n",
    "        output = response[\"choices\"][0][\"text\"]\n",
    "        obj = parser.parse(output)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"setup\": \"Why was the math book sad?\",\n",
      "\"punchline\": \"Because it had too many problems.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the math book sad?', punchline='Because it had too many problems.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Class as LlamaCppCustom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# llama2_model_file = \"../backend/data/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "# llama2_model_file = \"../backend/data/llama-2-13b-chat.ggmlv3.q4_K_M.bin\"\n",
    "# llama2_model_file = \"../backend/data/llama-2-13b-chat.ggmlv3.q5_K_M.bin\"\n",
    "llama2_model_file = \"../backend/data/llama-2-70b-chat.ggmlv3.q4_K_M.bin\"\n",
    "# llama2_model_file = \"../backend/data/llama-2-70b-chat.ggmlv3.q5_K_M.bin\"\n",
    "pathlib.Path(llama2_model_file).exists()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../backend/data/llama-2-70b-chat.ggmlv3.q4_K_M.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size =    0.21 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce GTX 1080 Ti) as main device\n",
      "llama_model_load_internal: mem required  = 40540.46 MB (+ 1280.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 1600 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from app.llama2cpp.component.llama2cpp import LlamaCppCustom\n",
    "\n",
    "\n",
    "n_gqa = 8 if \"70b\" in llama2_model_file else 1\n",
    "llm = LlamaCppCustom(\n",
    "    model_path=llama2_model_file,\n",
    "    n_ctx=4096,\n",
    "    temperature=0,\n",
    "    max_tokens=640,\n",
    "    n_gqa=n_gqa,\n",
    "    n_gpu_layers=None,  # 16\n",
    "    n_batch=512,\n",
    "    verbose=True,\n",
    "    streaming=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from app.llama2cpp.component.outputparser import PydanticOutputParserCustom\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParserCustom(pydantic_object=Joke)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>Answer the user query.\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example,\n",
      "\n",
      "- for the schema\n",
      "```\n",
      "{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "```\n",
      "\n",
      "- the well-formatted JSON object you shoud make to output\n",
      "```\n",
      "{\"foo\": [\"bar\", \"baz\"]}\n",
      "```\n",
      "which is a well-formatted instance of the schema.\n",
      "The object ```{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}``` is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"title\": \"Joke\", \"type\": \"object\", \"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "\n",
      "Then, You have to create JSON object with output schema, to answer the user\n",
      "<</SYS>>\n",
      "Tell me a joke.[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"[INST]<<SYS>>Answer the user query.\\n{format_instructions}<</SYS>>\\n{query}[/INST]\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "print(_input.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 20090.84 ms\n",
      "llama_print_timings:      sample time =    13.82 ms /    32 runs   (    0.43 ms per token,  2316.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20090.69 ms /   299 tokens (   67.19 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:        eval time = 48350.62 ms /    31 runs   ( 1559.70 ms per token,     0.64 tokens per second)\n",
      "llama_print_timings:       total time = 68510.09 ms\n"
     ]
    }
   ],
   "source": [
    "text = _input.to_string()\n",
    "output = llm(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"setup\": \"Why was the math book sad?\",\n",
      "\"punchline\": \"Because it had too many problems.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the math book sad?', punchline='Because it had too many problems.')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"setup\": \"Why was the math book sad?\",\n",
      "\"punchline\": \"Because it had too many problems.\"\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 20090.84 ms\n",
      "llama_print_timings:      sample time =    13.75 ms /    32 runs   (    0.43 ms per token,  2327.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 50358.08 ms /    32 runs   ( 1573.69 ms per token,     0.64 tokens per second)\n",
      "llama_print_timings:       total time = 50430.47 ms\n"
     ]
    }
   ],
   "source": [
    "# NOTE: streaming mode\n",
    "\n",
    "_prompt = _input.to_string()\n",
    "\n",
    "tokens = []\n",
    "for token in llm.stream(prompt=_prompt):\n",
    "    tkn = token[\"choices\"][0][\"text\"]\n",
    "    tokens.append(tkn)\n",
    "    print(tkn, sep=\"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the math book sad?', punchline='Because it had too many problems.')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

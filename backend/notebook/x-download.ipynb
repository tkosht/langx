{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../data/tweet_activity_metrics_tkosht_20240106_20240203_ja.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "url = \"https://twitter.com/ai_database/status/1756147762822058066\"\n",
    "print(f\"{url=}\")\n",
    "parsed = urlparse(url)\n",
    "\n",
    "url_robots = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "print(f\"{url_robots=}\")\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/121.0.6167.85 Safari/537.36\"\n",
    "rbp = RobotFileParser(url=url_robots)\n",
    "rbp.read()\n",
    "rbp.can_fetch(user_agent, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = rbp.entries[0]\n",
    "rule = entry.rulelines[0]\n",
    "rule.allowance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"PATH\"].split(\":\")[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.binary_location = \"/opt/chrome-linux64/chrome\"\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--enable-chrome-browser-cloud-management\")\n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "    service = Service(\"/usr/local/bin/chromedriver\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def fetch_text_with_selenium(url: str, finds: list = [], max_tries: int = 3):\n",
    "    driver = create_driver()\n",
    "\n",
    "    max_wait = 13\n",
    "    text = None\n",
    "    try:\n",
    "        for n_try in range(max_tries):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "\n",
    "                # wait appearance\n",
    "                for finder in finds:\n",
    "                    # eg) By.XPATH, \"//div\", \"copyright\" = rec : (str, str, str)\n",
    "                    by, by_elm, finder_text = finder\n",
    "\n",
    "                    try:\n",
    "                        WebDriverWait(driver, max_wait).until(\n",
    "                            expected_conditions.text_to_be_present_in_element(\n",
    "                                (by, by_elm), text_=finder_text\n",
    "                            )\n",
    "                        )\n",
    "                        text = driver.find_element(by=by, value=by_elm).text\n",
    "                        break\n",
    "                    except TimeoutException:\n",
    "                        continue\n",
    "\n",
    "                if not text:\n",
    "                    # browser.implicitly_wait(10)\n",
    "                    text = driver.find_element(by=By.TAG_NAME, value=\"body\").text\n",
    "                return text\n",
    "            except TimeoutException as te:\n",
    "                if n_try > max_tries:\n",
    "                    print(f\"[WARNING] fetching text was failed. [{url=}]\")\n",
    "                    raise te\n",
    "                t = 2**n_try\n",
    "                print(f\"sleeping to retry ... [{t=} secs]\")\n",
    "                time.sleep(t)\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = create_driver()\n",
    "# driver.execute_script(\"return navigator.userAgent;\")\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firefox\n",
    "# # NOTE: https://www.selenium.dev/ja/documentation/webdriver/browsers/firefox/\n",
    "# #       collaborated with ChatGPT-4\n",
    "# import subprocess\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.firefox.options import Options\n",
    "# from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "# from selenium.webdriver.common.by import By\n",
    "#\n",
    "#\n",
    "# def fetch_text_with_selenium(url):\n",
    "#     firefox_profile = FirefoxProfile()\n",
    "#     firefox_profile.set_preference(\"javascript.enabled\", True)\n",
    "#     options = Options()\n",
    "#     options.profile = firefox_profile\n",
    "#     options.add_argument(\"--headless\")\n",
    "#     options.binary_location = \"/usr/bin/firefox\"\n",
    "#     service = webdriver.FirefoxService(log_output=subprocess.STDOUT)\n",
    "#     driver = webdriver.Firefox(options=options, service=service)\n",
    "#\n",
    "#     try:\n",
    "#         # 指定されたURLにアクセス\n",
    "#         driver.get(url)\n",
    "#\n",
    "#         # ページの全テキストを抽出\n",
    "#         text = driver.find_element(by=\"tag name\", value=\"body\").text\n",
    "#         return text\n",
    "#     finally:\n",
    "#         # ブラウザを閉じる\n",
    "#         driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://example.com\"\n",
    "text = fetch_text_with_selenium(url)\n",
    "print(\"-\" * 120)\n",
    "print(text)\n",
    "print(\"-\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://twitter.com/ai_database/status/1756147762822058066\"\n",
    "xpath = '//div[@id=\"react-root\"]'\n",
    "text = fetch_text_with_selenium(url, finds=[(By.XPATH, xpath, \"Likes\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaning of twitter message\n",
    "def cleanup_text(text: str):\n",
    "    if text is None:\n",
    "        return None\n",
    "    if \"Conversation\" not in text:\n",
    "        # NOTE: possibly not x post\n",
    "        return text\n",
    "\n",
    "    do_skip = True\n",
    "    filtered_lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line == \"Conversation\":\n",
    "            do_skip = False\n",
    "            continue\n",
    "        if do_skip:\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "    cleaned_text = \"\\n\".join(filtered_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "clean_text = cleanup_text(text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Generated by ChatGPT-4\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_urls(text):\n",
    "    # URLの正規表現パターン\n",
    "    url_pattern = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "\n",
    "# テスト用のテキスト\n",
    "text = \"この文には2つのURLが含まれています。https://www.example.com と http://www.test.com です。\"\n",
    "\n",
    "# URLを抽出\n",
    "urls = extract_urls(text)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickup_reference_text(url: str, finds: list = [], max_tries: int = 1) -> None | str:\n",
    "    text = fetch_text_with_selenium(url, finds, max_tries=max_tries)\n",
    "    text = cleanup_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for X posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "to_ref = {}\n",
    "refs = []\n",
    "for idx, row in df[[\"ツイート本文\"]].iterrows():\n",
    "    text = row.iloc[0]\n",
    "    urls = extract_urls(text)\n",
    "    if not urls:\n",
    "        # url, ref_text\n",
    "        refs.append((None, None))\n",
    "        continue\n",
    "\n",
    "    # NOTE: pickup the last url\n",
    "    url = urls[-1]\n",
    "    print(f\"Processing [{url=}]\", end=\"\")\n",
    "\n",
    "    finds = [\n",
    "        (By.XPATH, '//div[@id=\"react-root\"]', \"Likes\"),\n",
    "        (By.TAG_NAME, \"body\", \"\\n\"),\n",
    "        (By.TAG_NAME, \"body\", \"Copyright\"),\n",
    "    ]\n",
    "\n",
    "    ref_text = pickup_reference_text(url, finds)\n",
    "    to_ref[url] = ref_text\n",
    "    refs.append((url, ref_text))\n",
    "\n",
    "    # if idx > 0:\n",
    "    #     break\n",
    "    t = np.random.randint(3, 10)\n",
    "    print(f\" ... sleeping {t} secs\")\n",
    "    time.sleep(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_text = pickup_reference_text(url=\"https://t.co/Vrg02hwcE1\", finds=finds)\n",
    "# ref_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "tsv_file = \"../data/x_refs.tsv\"\n",
    "pd.DataFrame(refs, columns=[\"url\", \"text\"]).to_csv(tsv_file, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(tsv_file, sep=\"\\t\", header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
